{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e587349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS:\n",
    "\n",
    "vocabulary_size = 100\n",
    "pad_token_index = 0\n",
    "batch_size = 2\n",
    "sequence_length = 4\n",
    "embedding_dimension = 8\n",
    "head_dimension = 4\n",
    "\n",
    "np.random.seed(0)\n",
    "# Sequence length is the number of tokens in sentence\n",
    "# Embedding dimension is the size of each token vector, which means each word will have 8 traits.\n",
    "# These are saved in token embeddings.\n",
    "\n",
    "# Head dimension means, size of each head dimension, attention uses  multiples head, that is like parallel brains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "447b63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN EMBEDDINGS:\n",
    "token_embeddings = np.random.randn(sequence_length, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5d35d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING 3 DiFFERENT PRESPECTIVES FOR EACH TOKEN\n",
    "# Query, Key, Value:\n",
    "# These are weight metrices, in my notebook, these are randomm. But in transformer model, learned during training.\n",
    "# Each has shape (8, 4). They are  like linear layers (y = x * W) that project embeddings into new space.\n",
    "# Query (Q): what I am  looking for.\n",
    "# Key (K): What can I offer??\n",
    "# Value (V): What is my actual content?\n",
    "\n",
    "# Why three?, because in conversations:\n",
    "# You ask questions (Q)\n",
    "# Others have answers (K)\n",
    "# everyone has their own story (V)\n",
    "\n",
    "# Each word becomes question asker \n",
    "W_q = np.random.randn(embedding_dimension, head_dimension)\n",
    "# Each word becomes answer giver\n",
    "W_k = np.random.randn(embedding_dimension, head_dimension)\n",
    "# Each word becomes actual content\n",
    "W_v = np.random.randn(embedding_dimension, head_dimension)\n",
    "# This is like asking: For each word, what is its query vector, what it wants from others?\n",
    "# It is vector, what it can offer?\n",
    "# It is value vector it is useful information? \n",
    "\n",
    "# Why in  attention?:\n",
    "# We  compare queries to keys to find matches, like a search engine.\n",
    "# Values are what we collect from good matches.\n",
    "# Projecting with W  lets the model learn different aspects Q/K for similarity, V for actual content.\n",
    "\n",
    "Q = token_embeddings @ W_q  \n",
    "K = token_embeddings @ W_k \n",
    "V = token_embeddings @ W_v \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09cb62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENSION SCORE: \n",
    "# Measures how much each token should attend to every other token, including itself this is called self attention.\n",
    "scale = np.sqrt(head_dimension)\n",
    "attention_scores = (Q @ K.T) / scale\n",
    "# Word1 asks Q1, Word2 answers with K2 calculate match score\n",
    "# Higher score = We understand each other well\n",
    "# Divide by sqrt(number of head) to keep scores from getting too big.\n",
    "\n",
    "# Word1 asks: I like cats\n",
    "# Word2 Key: I have cat stories\n",
    "# Word3 Key: I like dogs\n",
    "# Word4 Key: The weather is nice\n",
    "\n",
    "# For example, scores: Word1 with Word2 = 0.9 which is high, Word1 with Word3 = 0.5, Word1 with Word4 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222787d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION WEIGHT:\n",
    "# Here, raw score is turned into probabilities likesaying pay X%  attention here\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(attention_scores, axis=-1)\n",
    "\n",
    "# Softmax makes things differentiable and focuses on high scores while ignoring lows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c203112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING NEW UNDERSTANDING:\n",
    "# Here, each word creates a new version of itself by mixing other words information.\n",
    "# The model learns contextual meaning\n",
    "# Each word becomes smarter by considering its neighbors \n",
    "# For example: word bank can learn from account,  bank account.\n",
    "attention_output = attention_weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources: \n",
    "# https://medium.com/%40wangdk93/multihead-attention-from-scratch-6fd6f99b9651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37427ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SELF-ATTENTION MECHANISM - SHAPES AND VALUES\n",
      "============================================================\n",
      "\n",
      "1. TOKEN EMBEDDINGS:\n",
      " Shape: (4, 8) → (sequence_length=4, embedding_dim=8)\n",
      "Values:\n",
      "[[ 1.764  0.4    0.979  2.241  1.868 -0.977  0.95  -0.151]\n",
      " [-0.103  0.411  0.144  1.454  0.761  0.122  0.444  0.334]\n",
      " [ 1.494 -0.205  0.313 -0.854 -2.553  0.654  0.864 -0.742]\n",
      " [ 2.27  -1.454  0.046 -0.187  1.533  1.469  0.155  0.378]]\n",
      "\n",
      "2. WEIGHT MATRICES:\n",
      "W_q, W_k, W_v shape: (8, 4) → (embedding_dim=8, head_dim=4)\n",
      "\n",
      "3. QUERY, KEY, VALUE PROJECTIONS:\n",
      "Q, K, V shape: (4, 4) → (sequence_length=4, head_dim=4)\n",
      "Q (what each token looks for):\n",
      "[[-5.592 -4.286 -7.37   4.027]\n",
      " [-1.781 -0.435 -3.429  0.881]\n",
      " [ 2.871 -2.977  2.418  0.502]\n",
      " [-7.225 -8.378 -1.89   1.252]]\n",
      "\n",
      "4. ATTENTION SCORES (before softmax):\n",
      "Shape: (4, 4) → (seq_len × seq_len)\n",
      "Values:\n",
      "[[ 31.588   2.933  12.022  -9.08 ]\n",
      " [ 11.106   1.093   3.98    0.299]\n",
      " [ -8.946  -4.35    5.684  -5.326]\n",
      " [ 33.206   5.506  -4.183 -22.69 ]]\n",
      "\n",
      "5. ATTENTION WEIGHTS (after softmax):\n",
      "Shape: (4, 4)\n",
      "Values (each row sums to 1.0):\n",
      "[[1.    0.    0.    0.   ]\n",
      " [0.999 0.    0.001 0.   ]\n",
      " [0.    0.    1.    0.   ]\n",
      " [1.    0.    0.    0.   ]]\n",
      "Row sums (verification): [1. 1. 1. 1.]\n",
      "\n",
      "6. ATTENTION OUTPUT:\n",
      "Shape: (4, 4) → (sequence_length=4, head_dim=4)\n",
      "Values (context-aware representations):\n",
      "[[ 1.862 10.528  2.744  3.973]\n",
      " [ 1.861 10.516  2.742  3.968]\n",
      " [ 1.278 -3.528  0.392 -2.278]\n",
      " [ 1.862 10.528  2.744  3.973]]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SELF-ATTENTION MECHANISM - SHAPES AND VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. TOKEN EMBEDDINGS:\")\n",
    "print(f\" Shape: {token_embeddings.shape} → (sequence_length={sequence_length}, embedding_dim={embedding_dimension})\")\n",
    "print(f\"Values:\\n{token_embeddings}\\n\")\n",
    "\n",
    "print(\"2. WEIGHT MATRICES:\")\n",
    "print(f\"W_q, W_k, W_v shape: {W_q.shape} → (embedding_dim={embedding_dimension}, head_dim={head_dimension})\")\n",
    "\n",
    "print(\"\\n3. QUERY, KEY, VALUE PROJECTIONS:\")\n",
    "print(f\"Q, K, V shape: {Q.shape} → (sequence_length={sequence_length}, head_dim={head_dimension})\")\n",
    "print(f\"Q (what each token looks for):\\n{Q}\\n\")\n",
    "\n",
    "print(\"4. ATTENTION SCORES (before softmax):\")\n",
    "print(f\"Shape: {attention_scores.shape} → (seq_len × seq_len)\")\n",
    "print(f\"Values:\\n{attention_scores}\\n\")\n",
    "\n",
    "print(\"5. ATTENTION WEIGHTS (after softmax):\")\n",
    "print(f\"Shape: {attention_weights.shape}\")\n",
    "print(f\"Values (each row sums to 1.0):\\n{attention_weights}\")\n",
    "print(f\"Row sums (verification): {attention_weights.sum(axis=-1)}\\n\")\n",
    "\n",
    "print(\"6. ATTENTION OUTPUT:\")\n",
    "print(f\"Shape: {attention_output.shape} → (sequence_length={sequence_length}, head_dim={head_dimension})\")\n",
    "print(f\"Values (context-aware representations):\\n{attention_output}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# I got the code from Qwen Max for better clarity and visibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a4e5a",
   "metadata": {},
   "source": [
    "<h2><center>Multi Head Self attention and masking from scratch<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ce646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In here  I am going to create multi head attention from scratch,  multiple heads self attention mechanism is like having many brains.\n",
    "# from what I understand, it helps the model focus on different parts of the input parallelly.\n",
    "# In single head attention, I have one set of Q, K, V  per token. but here, I will have multiple sets of Q, K, V per token.\n",
    "# In LSTM time series, I tried to forecast the multivariate time series, zone1, zone2, zone3. zone3 got bad result\n",
    "# While I was looking on the internet, to handle  that issue, mulit head attention is suggested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't need to dine hyperparameters again, they are already defined above.\n",
    "# just small change:\n",
    "# Each head gets 4 traits to work with.\n",
    "multi_head_dimension = head_dimension // 2\n",
    "np.random.seed(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
