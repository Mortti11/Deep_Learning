{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff13fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import  precision_score, recall_score, f1_score,  roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5de074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I trained the CNN model in Keras framework already, I am going to build CNN in PyTorch framework to see, could I get better result.\n",
    "# I will use the same code structure for prepring data in here as in Keras notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b6a75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccebe953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Temperature",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Humidity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CO2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Occupancy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "weekend",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tod_sin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tod_cos",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7c53ac56-fee9-4395-99f5-8c7818da2380",
       "rows": [
        [
         "0",
         "23.18",
         "27.272",
         "721.25",
         "1",
         "0",
         "-0.9992290362407228",
         "-0.039259815759069"
        ],
        [
         "1",
         "23.15",
         "27.2675",
         "714.0",
         "1",
         "0",
         "-0.9992290362407228",
         "-0.039259815759069"
        ],
        [
         "2",
         "23.15",
         "27.245",
         "713.5",
         "1",
         "0",
         "-0.9995335908367128",
         "-0.0305385132098227"
        ],
        [
         "3",
         "23.15",
         "27.2",
         "708.25",
         "1",
         "0",
         "-0.9996573249755571",
         "-0.0261769483078734"
        ],
        [
         "4",
         "23.1",
         "27.2",
         "704.5",
         "1",
         "0",
         "-0.9997620270799092",
         "-0.0218148850345608"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>CO2</th>\n",
       "      <th>Occupancy</th>\n",
       "      <th>weekend</th>\n",
       "      <th>tod_sin</th>\n",
       "      <th>tod_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>721.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.999229</td>\n",
       "      <td>-0.039260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>714.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.999229</td>\n",
       "      <td>-0.039260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2450</td>\n",
       "      <td>713.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.999534</td>\n",
       "      <td>-0.030539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>708.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.999657</td>\n",
       "      <td>-0.026177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.10</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>704.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.999762</td>\n",
       "      <td>-0.021815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  Humidity     CO2  Occupancy  weekend   tod_sin   tod_cos\n",
       "0        23.18   27.2720  721.25          1        0 -0.999229 -0.039260\n",
       "1        23.15   27.2675  714.00          1        0 -0.999229 -0.039260\n",
       "2        23.15   27.2450  713.50          1        0 -0.999534 -0.030539\n",
       "3        23.15   27.2000  708.25          1        0 -0.999657 -0.026177\n",
       "4        23.10   27.2000  704.50          1        0 -0.999762 -0.021815"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the DSs \n",
    "df_test = pd.read_csv('cleaned_datatest.csv')\n",
    "df_val = pd.read_csv('datavalidation.csv')\n",
    "df_train = pd.read_csv('datatraining.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3162e4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(df_train.duplicated().sum())\n",
    "print(df_val.duplicated().sum())\n",
    "print(df_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a3c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a3d66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (8117, 6) (8117,)\n",
      "Val: (2663, 6) (2663,)\n",
      "Test: (9720, 6) (9720,)\n"
     ]
    }
   ],
   "source": [
    "# Preparing data for CNN model,\n",
    "target = 'Occupancy'\n",
    "features = [\"Temperature\", \"Humidity\", \"CO2\", \"weekend\", \"tod_sin\", \"tod_cos\"]\n",
    "\n",
    "X_train = df_train[features].values\n",
    "y_train = df_train[target].values.astype(int)\n",
    "\n",
    "X_val = df_val[features].values\n",
    "y_val = df_val[target].values.astype(int)\n",
    "\n",
    "X_test = df_test[features].values\n",
    "y_test = df_test[target].values.astype(int)\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Scaling the features, I will fit only the train  DS\n",
    "# Because the weekend is binary and tod_sin, tod_cos are already normialized, to -1 , 1. So I will Standardize the continuous features.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled[:, 0:3] = scaler.fit_transform(X_train_scaled[:, 0:3])\n",
    "\n",
    "# transforming val and test DS \n",
    "X_val_scaled[:, 0:3] = scaler.transform(X_val_scaled[:, 0:3])\n",
    "X_test_scaled[:, 0:3]= scaler.transform(X_test_scaled[:, 0:3])\n",
    "\n",
    "print(\"Train:\", X_train_scaled.shape, y_train.shape)\n",
    "print(\"Val:\", X_val_scaled.shape, y_val.shape)\n",
    "print(\"Test:\", X_test_scaled.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153e0047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekend unique: [0. 1.]\n",
      "tod_sin min/max: -1.0 1.0\n",
      "tod_cos min/max: -1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Checking the weekend and tod_sin, tod_cos. \n",
    "print(\"weekend unique:\", np.unique(X_train_scaled[:,3]))\n",
    "print(\"tod_sin min/max:\", X_train_scaled[:,4].min(), X_train_scaled[:,4].max())\n",
    "print(\"tod_cos min/max:\", X_train_scaled[:,5].min(), X_train_scaled[:,5].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2edb1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train windows: (8057, 60, 6) (8057,)\n",
      "Val windows: (2603, 60, 6) (2603,)\n",
      "Test windows: (9660, 60, 6) (9660,)\n"
     ]
    }
   ],
   "source": [
    "# MAKING SEQUENCES:\n",
    "# The CNN model for time series, needs sequences as input \n",
    "# I will create windows of minutes that the model will predict right after the window, the next minute\n",
    "\n",
    "# Creating function\n",
    "\n",
    "window_size = 60\n",
    "step_ahead = 1\n",
    "def windows_time(X, y, window_size = None, step_ahead = None):\n",
    "   # I will creaate two lists for storing  features and the target\n",
    "   # The end should start the a window\n",
    "   X_windows = []\n",
    "   y_windows = []\n",
    "   last_start = len(X) - window_size - step_ahead + 1\n",
    "   for start in range(last_start):\n",
    "        end = start + window_size\n",
    "        X_windows.append(X[start:end])                  \n",
    "        y_windows.append(y[end + step_ahead - 1])      \n",
    "   return np.array(X_windows), np.array(y_windows)\n",
    "\n",
    "  \n",
    "X_train_window, y_train_window = windows_time(X_train_scaled, y_train, window_size, step_ahead)\n",
    "X_val_window, y_val_window = windows_time(X_val_scaled, y_val, window_size, step_ahead)\n",
    "X_test_window, y_test_window = windows_time(X_test_scaled, y_test, window_size, step_ahead)\n",
    "\n",
    "print(\"Train windows:\", X_train_window.shape, y_train_window.shape)\n",
    "print(\"Val windows:\", X_val_window.shape,y_val_window.shape)\n",
    "print(\"Test windows:\", X_test_window.shape, y_test_window.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b86ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_window, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_window, dtype=torch.long) \n",
    "X_val_tensor = torch.tensor(X_val_window, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_window, dtype=torch.long)\n",
    "X_test_tensor= torch.tensor(X_test_window, dtype=torch.float32)\n",
    "y_test_tensor= torch.tensor(y_test_window, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ba580d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504a9af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([128, 60, 6])\n",
      "Batch target shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# I nned to see shape of one batch\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(f\"Batch input shape: {X_batch.shape}\") \n",
    "    print(f\"Batch target shape: {y_batch.shape}\")\n",
    "    break  \n",
    "# In forward function, I will switch input shape and sequence lemght"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b82d6",
   "metadata": {},
   "source": [
    "<h2><center>CNN Architecture<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680f1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to build 1D CNN for time series classification. Same issue here, the imbalanced rate is not same for each DS.\n",
    "# First I need to create a convolutional block class \n",
    "# Second, I am going to create the CNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnvectionalBlocks(nn.Module):\n",
    "    def __init__(self, Input_channel, Output_channel, kernel_size=3, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # First convolutional layer:\n",
    "        # the constrcutor will have input channel, output channel, kernel size and dropout rate as parameters.\n",
    "        self.conv1 = nn.Conv1d(in_channels=Input_channel, out_channels=Output_channel, kernel_size=kernel_size, padding= kernel_size//2)\n",
    "        # Batch normalization:\n",
    "        self.bn1 = nn.BatchNorm1d(Output_channel)\n",
    "        self.drop = nn.Dropout1d(p=dropout_rate)\n",
    "\n",
    "# Operation function:\n",
    " # Extrating the local patters \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First convolutional block\n",
    "        self.conv_block1 = ConnvectionalBlocks(Input_channel=6, Output_channel=32, kernel_size=3, dropout_rate=0.1)\n",
    "        # Second convolutional block\n",
    "        self.conv_block2 = ConnvectionalBlocks(Input_channel=32, Output_channel= 32, kernel_size=3, dropout_rate=0.1)\n",
    "        # Max pooling, to halve the lengh\n",
    "        self.pool1  = nn.MaxPool1d(kernel_size=1)\n",
    "\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv_block3 = ConnvectionalBlocks(Input_channel=32, Output_channel=32, kernel_size=3, dropout_rate=0.2)\n",
    "        # Fourth convolutional block\n",
    "        self.conv_block4 = ConnvectionalBlocks(Input_channel=32, Output_channel=32, kernel_size=3, dropout_rate=0.2)\n",
    "        # second Max pooling\n",
    "        self.pool2  = nn.MaxPool1d(kernel_size=1)\n",
    "\n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(32, 1)\n",
    "\n",
    "\n",
    "# Operation function:   \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        logits = self.fc1(x)\n",
    "        return logits\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Sources :\n",
    "# https://medium.com/@ugamakelechi501/building-a-convolutional-neural-network-cnn-from-scratch-with-pytorch-eca3ffdcf2ff\n",
    "# https://www.dkneup.com/blog/cnn-time-series-forecasting-in-tensorflow-pytorch\n",
    "# https://medium.com/%40santi.pdp/how-pytorch-transposed-convs1d-work-a7adac63c4a5\n",
    "# For debugging, I used the Qwen3 Max model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38269df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in keras notebook, Calculating class weights does not work well here.\n",
    "# The gap between training and validation loss is big, because \n",
    "# the distribution of Occupancy is different in each DS, because the model is trained on train DS distribution but\n",
    "# evaluated on val DS distribution.\n",
    "\n",
    "# I should go for same way as in Keras notebook, Using Focal Loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c174a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Focal Loss class\n",
    "# I am going to define hyperparameters: alpha which is for controlling the balance.\n",
    "# Gamma is focus parameter. Reduction is for aggregating the loss values.\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=3.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.float()\n",
    "        binary_class_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        probability_true_class  = torch.exp(-binary_class_loss)\n",
    "        loss = (1 - probability_true_class).pow(self.gamma) * binary_class_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# Usage - NO dataset-specific weights needed!\n",
    "criterion = FocalLoss(gamma=3.0)\n",
    "\n",
    "# Sources:\n",
    "# https://discuss.pytorch.org/t/implementing-focal-loss-for-a-binary-classification-problem/128664\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6454a297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 5e-06\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000005)\n",
    "# If val loss does not improve for 5 epochs, lr is going to be cut by half\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3, mode='max')\n",
    "print('Optimizer:', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b19e6801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/300 | TrainLoss 0.0325 Acc 0.914 | ValLoss 0.0235 AP 0.9429 | P 0.891 R 0.974 F1 0.930 | Thr 0.499 | LR 0.001000\n",
      "Epoch   2/300 | TrainLoss 0.0188 Acc 0.949 | ValLoss 0.0279 AP 0.9191 | P 0.856 R 0.843 F1 0.850 | Thr 0.499 | LR 0.001000\n",
      "Epoch   3/300 | TrainLoss 0.0167 Acc 0.954 | ValLoss 0.0276 AP 0.9306 | P 0.897 R 0.781 F1 0.835 | Thr 0.499 | LR 0.001000\n",
      "Epoch   4/300 | TrainLoss 0.0150 Acc 0.957 | ValLoss 0.0307 AP 0.9296 | P 0.915 R 0.672 F1 0.775 | Thr 0.499 | LR 0.001000\n",
      "Epoch   5/300 | TrainLoss 0.0144 Acc 0.959 | ValLoss 0.0251 AP 0.9427 | P 0.912 R 0.796 F1 0.850 | Thr 0.499 | LR 0.001000\n",
      "Epoch   6/300 | TrainLoss 0.0140 Acc 0.961 | ValLoss 0.0293 AP 0.9445 | P 0.889 R 0.955 F1 0.921 | Thr 0.413 | LR 0.000500\n",
      "Epoch   7/300 | TrainLoss 0.0129 Acc 0.962 | ValLoss 0.0283 AP 0.9494 | P 0.890 R 0.955 F1 0.921 | Thr 0.409 | LR 0.000500\n",
      "Epoch   8/300 | TrainLoss 0.0135 Acc 0.963 | ValLoss 0.0280 AP 0.9328 | P 0.850 R 0.970 F1 0.906 | Thr 0.409 | LR 0.000500\n",
      "Epoch   9/300 | TrainLoss 0.0125 Acc 0.965 | ValLoss 0.0249 AP 0.9519 | P 0.867 R 0.989 F1 0.924 | Thr 0.351 | LR 0.000500\n",
      "Epoch  10/300 | TrainLoss 0.0133 Acc 0.966 | ValLoss 0.0253 AP 0.9450 | P 0.821 R 0.990 F1 0.898 | Thr 0.351 | LR 0.000500\n",
      "Epoch  11/300 | TrainLoss 0.0117 Acc 0.968 | ValLoss 0.0295 AP 0.9556 | P 0.900 R 0.959 F1 0.929 | Thr 0.377 | LR 0.000500\n",
      "Epoch  12/300 | TrainLoss 0.0114 Acc 0.967 | ValLoss 0.0284 AP 0.9550 | P 0.879 R 0.932 F1 0.905 | Thr 0.377 | LR 0.000500\n",
      "Epoch  13/300 | TrainLoss 0.0114 Acc 0.969 | ValLoss 0.0373 AP 0.9470 | P 0.883 R 0.856 F1 0.870 | Thr 0.377 | LR 0.000500\n",
      "Epoch  14/300 | TrainLoss 0.0111 Acc 0.969 | ValLoss 0.0287 AP 0.9549 | P 0.893 R 0.955 F1 0.923 | Thr 0.377 | LR 0.000500\n",
      "Epoch  15/300 | TrainLoss 0.0112 Acc 0.968 | ValLoss 0.0278 AP 0.9571 | P 0.887 R 0.978 F1 0.930 | Thr 0.347 | LR 0.000500\n",
      "Epoch  16/300 | TrainLoss 0.0113 Acc 0.970 | ValLoss 0.0298 AP 0.9530 | P 0.869 R 0.966 F1 0.915 | Thr 0.347 | LR 0.000500\n",
      "Epoch  17/300 | TrainLoss 0.0108 Acc 0.971 | ValLoss 0.0453 AP 0.9478 | P 0.882 R 0.840 F1 0.861 | Thr 0.347 | LR 0.000500\n",
      "Epoch  18/300 | TrainLoss 0.0103 Acc 0.971 | ValLoss 0.0456 AP 0.9388 | P 0.861 R 0.845 F1 0.853 | Thr 0.347 | LR 0.000500\n",
      "Epoch  19/300 | TrainLoss 0.0116 Acc 0.968 | ValLoss 0.0302 AP 0.9537 | P 0.878 R 0.958 F1 0.916 | Thr 0.347 | LR 0.000500\n",
      "Epoch  20/300 | TrainLoss 0.0113 Acc 0.969 | ValLoss 0.0357 AP 0.9399 | P 0.843 R 0.927 F1 0.883 | Thr 0.347 | LR 0.000250\n",
      "Epoch  21/300 | TrainLoss 0.0110 Acc 0.969 | ValLoss 0.0440 AP 0.9377 | P 0.843 R 0.846 F1 0.845 | Thr 0.347 | LR 0.000250\n",
      "Epoch  22/300 | TrainLoss 0.0104 Acc 0.970 | ValLoss 0.0503 AP 0.9412 | P 0.868 R 0.826 F1 0.846 | Thr 0.347 | LR 0.000250\n",
      "Epoch  23/300 | TrainLoss 0.0104 Acc 0.974 | ValLoss 0.0506 AP 0.9444 | P 0.879 R 0.826 F1 0.851 | Thr 0.347 | LR 0.000250\n",
      "Epoch  24/300 | TrainLoss 0.0104 Acc 0.972 | ValLoss 0.0347 AP 0.9453 | P 0.865 R 0.953 F1 0.907 | Thr 0.347 | LR 0.000125\n",
      "Epoch  25/300 | TrainLoss 0.0105 Acc 0.969 | ValLoss 0.0506 AP 0.9451 | P 0.875 R 0.825 F1 0.849 | Thr 0.347 | LR 0.000125\n",
      "Early stopping triggered.\n",
      "Best Val AP: 0.9571 | Best Val Threshold: 0.347\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "num_epochs = 300\n",
    "patience = 10\n",
    "min_delta = 1e-4\n",
    "default_threshold = 0.5\n",
    "\n",
    "best_val_ap = -float(\"inf\")\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "best_threshold = default_threshold\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    # ----------------------------\n",
    "    # TRAIN\n",
    "    # ----------------------------\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch_features, batch_targets in train_loader:\n",
    "        batch_features = batch_features.to(device)   # (B, 60, 6)\n",
    "        batch_targets  = batch_targets.to(device)    # (B,)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        batch_logits = model(batch_features).squeeze(1)               # (B,)\n",
    "        batch_loss   = criterion(batch_logits, batch_targets.float()) # loss expects float targets\n",
    "        batch_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "        batch_probs = torch.sigmoid(batch_logits)\n",
    "        batch_preds = (batch_probs >= default_threshold).to(torch.int64)\n",
    "\n",
    "        train_correct += (batch_preds == batch_targets).sum().item()\n",
    "        train_total   += batch_targets.size(0)\n",
    "\n",
    "    train_loss = train_loss_sum / len(train_dataset)\n",
    "    train_acc  = train_correct / train_total\n",
    "\n",
    "    # ----------------------------\n",
    "    # VALIDATION\n",
    "    # ----------------------------\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_probs_chunks = []\n",
    "    val_targets_chunks = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_features, batch_targets in val_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_targets  = batch_targets.to(device)\n",
    "\n",
    "            batch_logits = model(batch_features).squeeze(1)\n",
    "            batch_loss   = criterion(batch_logits, batch_targets.float())\n",
    "            val_loss_sum += batch_loss.item() * batch_features.size(0)\n",
    "\n",
    "            batch_probs = torch.sigmoid(batch_logits)\n",
    "            val_probs_chunks.append(batch_probs.cpu().numpy())\n",
    "            val_targets_chunks.append(batch_targets.cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss_sum / len(val_dataset)\n",
    "\n",
    "    val_probs   = np.concatenate(val_probs_chunks).astype(float)\n",
    "    val_targets = np.concatenate(val_targets_chunks).astype(int)\n",
    "\n",
    "    # AUPRC / AP (threshold-free)\n",
    "    val_ap = average_precision_score(val_targets, val_probs)\n",
    "\n",
    "    # ----------------------------\n",
    "    # THRESHOLD SEARCH (only if AP improved enough)\n",
    "    # ----------------------------\n",
    "    new_best_threshold = best_threshold\n",
    "    if val_ap > best_val_ap + min_delta:\n",
    "        prec, rec, thr = precision_recall_curve(val_targets, val_probs)\n",
    "\n",
    "        # thr length == len(prec)-1, so align\n",
    "        prec_t = prec[:-1]\n",
    "        rec_t  = rec[:-1]\n",
    "\n",
    "        f1_curve = (2 * prec_t * rec_t) / (prec_t + rec_t + 1e-12)\n",
    "        best_i = int(np.nanargmax(f1_curve))\n",
    "        new_best_threshold = float(thr[best_i])\n",
    "\n",
    "    val_pred_labels = (val_probs >= new_best_threshold).astype(int)\n",
    "    val_precision = precision_score(val_targets, val_pred_labels, zero_division=0)\n",
    "    val_recall    = recall_score(val_targets, val_pred_labels, zero_division=0)\n",
    "    val_f1        = f1_score(val_targets, val_pred_labels, zero_division=0)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:3d}/{num_epochs} | \"\n",
    "        f\"TrainLoss {train_loss:.4f} Acc {train_acc:.3f} | \"\n",
    "        f\"ValLoss {val_loss:.4f} AP {val_ap:.4f} | \"\n",
    "        f\"P {val_precision:.3f} R {val_recall:.3f} F1 {val_f1:.3f} | \"\n",
    "        f\"Thr {new_best_threshold:.3f} | \"\n",
    "        f\"LR {current_lr:.6f}\"\n",
    "    )\n",
    "\n",
    "  \n",
    "    scheduler.step(val_ap)\n",
    "    if val_ap > best_val_ap + min_delta:\n",
    "        best_val_ap = val_ap\n",
    "        best_threshold = new_best_threshold\n",
    "        patience_counter = 0\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "print(f\"Best Val AP: {best_val_ap:.4f} | Best Val Threshold: {best_threshold:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef51b23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST AP (AUPRC): 0.6657\n",
      "TEST threshold: 0.347\n",
      "Confusion matrix:\n",
      " [[6826  842]\n",
      " [ 377 1615]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Occupancy       0.95      0.89      0.92      7668\n",
      "   Occupancy       0.66      0.81      0.73      1992\n",
      "\n",
      "    accuracy                           0.87      9660\n",
      "   macro avg       0.80      0.85      0.82      9660\n",
      "weighted avg       0.89      0.87      0.88      9660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "test_probs_list = []\n",
    "test_targets_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_targets in test_loader:\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_targets  = batch_targets.to(device)\n",
    "\n",
    "        batch_logits = model(batch_features).squeeze(1)\n",
    "        batch_probs = torch.sigmoid(batch_logits)\n",
    "\n",
    "        test_probs_list.append(batch_probs.detach().cpu().numpy())\n",
    "        test_targets_list.append(batch_targets.detach().cpu().numpy())\n",
    "\n",
    "test_probs = np.concatenate(test_probs_list).astype(float)\n",
    "test_targets = np.concatenate(test_targets_list).astype(int)\n",
    "\n",
    "test_ap = average_precision_score(test_targets, test_probs)\n",
    "test_preds = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "print(\"TEST AP (AUPRC):\", round(test_ap, 4))\n",
    "print(\"TEST threshold:\", round(best_threshold, 3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(test_targets, test_preds))\n",
    "print(classification_report(test_targets, test_preds, target_names=[\"No Occupancy\",\"Occupancy\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7871d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/300 | TrainLoss 0.0063 Acc 0.967 | ValLoss 0.0109 AP 0.9581 | P 0.905 R 0.978 F1 0.940 | Thr 0.437 | LR 0.000125\n",
      "Epoch   2/300 | TrainLoss 0.0062 Acc 0.969 | ValLoss 0.0113 AP 0.9457 | P 0.866 R 0.978 F1 0.919 | Thr 0.437 | LR 0.000125\n",
      "Epoch   3/300 | TrainLoss 0.0057 Acc 0.969 | ValLoss 0.0107 AP 0.9554 | P 0.896 R 0.978 F1 0.935 | Thr 0.437 | LR 0.000063\n",
      "Epoch   4/300 | TrainLoss 0.0061 Acc 0.968 | ValLoss 0.0119 AP 0.9435 | P 0.875 R 0.970 F1 0.920 | Thr 0.437 | LR 0.000063\n",
      "Epoch   5/300 | TrainLoss 0.0060 Acc 0.971 | ValLoss 0.0115 AP 0.9491 | P 0.886 R 0.970 F1 0.926 | Thr 0.437 | LR 0.000063\n",
      "Epoch   6/300 | TrainLoss 0.0060 Acc 0.970 | ValLoss 0.0117 AP 0.9574 | P 0.912 R 0.961 F1 0.935 | Thr 0.437 | LR 0.000063\n",
      "Epoch   7/300 | TrainLoss 0.0059 Acc 0.968 | ValLoss 0.0113 AP 0.9539 | P 0.896 R 0.968 F1 0.930 | Thr 0.437 | LR 0.000031\n",
      "Epoch   8/300 | TrainLoss 0.0057 Acc 0.970 | ValLoss 0.0121 AP 0.9497 | P 0.883 R 0.931 F1 0.906 | Thr 0.437 | LR 0.000031\n",
      "Epoch   9/300 | TrainLoss 0.0056 Acc 0.971 | ValLoss 0.0106 AP 0.9573 | P 0.901 R 0.975 F1 0.936 | Thr 0.437 | LR 0.000031\n",
      "Epoch  10/300 | TrainLoss 0.0059 Acc 0.969 | ValLoss 0.0120 AP 0.9529 | P 0.896 R 0.932 F1 0.913 | Thr 0.437 | LR 0.000031\n",
      "Epoch  11/300 | TrainLoss 0.0058 Acc 0.969 | ValLoss 0.0119 AP 0.9508 | P 0.889 R 0.963 F1 0.924 | Thr 0.437 | LR 0.000016\n",
      "Early stopping triggered.\n",
      "Best Val AP: 0.9581 | Best Val Threshold: 0.437\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "best_val_loss = 0\n",
    "patience = 8\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train() \n",
    "    train_loss_sum = 0\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X_batch)            \n",
    "        loss = criterion(outputs.squeeze(1), y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_loss_sum += loss.item() * X_batch.size(0)\n",
    "        preds = (torch.sigmoid(outputs) >= default_threshold).float() \n",
    "        correct_predictions += (preds.squeeze() == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    train_loss = train_loss_sum / len(train_dataset)\n",
    "    train_acc = correct_predictions / total\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "      \n",
    "     \n",
    "    with torch.no_grad():  \n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits.squeeze(1), y_batch.float())\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "\n",
    "            probs = torch.sigmoid(logits.squeeze(1))\n",
    "            preds = (probs >= 0.611).to(torch.int64)\n",
    "\n",
    "\n",
    "            val_preds.extend(preds.tolist())\n",
    "            val_targets.extend(y_batch.tolist())\n",
    "            \n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "\n",
    "\n",
    "    \n",
    "    # Converting predictions and targets to numpy for calculations\n",
    "    val_preds = np.array(val_preds)\n",
    "    val_targets = np.array(val_targets)\n",
    "    \n",
    "    val_prec = average_precision_score(val_targets, val_targets)\n",
    "    learning_rate = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d}/{epochs} - \"f\"Train Loss: {train_loss:.4f} \"f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Prec: {val_prec:.3f}, \"f\"Learning Rate: {learning_rate:.6f}\")\n",
    "    \n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        \n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "if patience_counter >= patience:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Source: https://docs.pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36093e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits.squeeze(), y_batch.float())\n",
    "        probs = torch.sigmoid(logits.squeeze())\n",
    "        preds = (probs >=best_threshold).to(torch.int64)\n",
    "        test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        test_preds.extend(preds.tolist())\n",
    "        test_targets.extend(y_batch.tolist())\n",
    "test_preds = np.array(test_preds)\n",
    "test_targets = np.array(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf49ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Confusion_matrix = confusion_matrix(test_targets, test_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "sns.heatmap(Confusion_matrix, annot=True, fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddafa47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "No Occupancy       0.97      0.89      0.93      7668\n",
      "   Occupancy       0.69      0.89      0.77      1992\n",
      "\n",
      "    accuracy                           0.89      9660\n",
      "   macro avg       0.83      0.89      0.85      9660\n",
      "weighted avg       0.91      0.89      0.90      9660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Classification_report = classification_report(test_targets, test_preds, target_names=['No Occupancy', 'Occupancy'], zero_division=0)\n",
    "print(\"Classification Report:\\n\", Classification_report)\n",
    "\n",
    "# Even though I have calculated class weights, but still the model is biased towards the class occupancy.\n",
    "# The gap between traning and validation loss is big, because the distribution of Occupancy is different in each DS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
